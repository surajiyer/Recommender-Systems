{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Microsoft COCO (Common Objects in Context) data set to train our \"Image Caption Retrieval Model\". This data set consists of pretrained 10-crop VGG19 features (Neural codes) and its corresponding text caption. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embeddings'\n",
    "MODEL_PATH = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create above directories and locate data set provided in directory 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading pairs of image (VGG19 features) - caption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "import collections\n",
    "\n",
    "np_train_data = np.load(os.path.join(DATA_PATH,'train_data.npy'))\n",
    "np_val_data = np.load(os.path.join(DATA_PATH,'val_data.npy'))\n",
    "\n",
    "train_data = collections.OrderedDict()\n",
    "for i in range(len(np_train_data.item())):\n",
    "    cap =  np_train_data.item()['caps']\n",
    "    img =  np_train_data.item()['ims']\n",
    "    train_data['caps'] = cap\n",
    "    train_data['ims'] = img\n",
    "    \n",
    "val_data = collections.OrderedDict()\n",
    "for i in range(len(np_val_data.item())):\n",
    "    cap =  np_val_data.item()['caps']\n",
    "    img =  np_val_data.item()['ims']\n",
    "    val_data['caps'] = cap\n",
    "    val_data['ims'] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of caption\n",
    "train_data['caps'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of pre-computed VGG19 features\n",
    "val_data['ims'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading caption and information about its corresponding raw images from Microsoft COCO website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "# use them for your own additional preprocessing step\n",
    "# to map precomputed features and location of raw images \n",
    "\n",
    "import json\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'instances_val2014.json')) as json_file:\n",
    "    coco_instances_val = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(DATA_PATH,'captions_val2014.json')) as json_file:\n",
    "    coco_caption_val = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your own function to map pairs of precomputed features and filepath of raw images\n",
    "# this will be used later for visualization part\n",
    "# simple approach: based on matched text caption (see json file)\n",
    "\n",
    "# YOUR CODE HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "def build_dictionary(text):\n",
    "\n",
    "    wordcount = OrderedDict()\n",
    "    for cc in text:\n",
    "        words = cc.split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 0\n",
    "            wordcount[w] += 1\n",
    "    words = list(wordcount.keys())\n",
    "    freqs = list(wordcount.values())\n",
    "    sorted_idx = np.argsort(freqs)[::-1]\n",
    "    \n",
    "\n",
    "    worddict = OrderedDict()\n",
    "    worddict['<pad>'] = 0\n",
    "    worddict['<unk>'] = 1\n",
    "    for idx, sidx in enumerate(sorted_idx):\n",
    "        worddict[words[sidx]] = idx+2  # 0: <pad>, 1: <unk>\n",
    "    \n",
    "\n",
    "    return worddict\n",
    "\n",
    "# use the resulting vocabulary index as your look up dictionary\n",
    "# to transform raw text into integer sequences\n",
    "\n",
    "all_captions = []\n",
    "all_captions = train_data['caps'] + val_data['caps']\n",
    "\n",
    "# decode bytes to string format\n",
    "caps = []\n",
    "for w in all_captions:\n",
    "    caps.append(w.decode())\n",
    "    \n",
    "words_indices = build_dictionary(caps)\n",
    "print ('Dictionary size: ' + str(len(words_indices)))\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image - Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# For embedding layer, initialize with pretrained word embedding (GloVe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# layer for computing dot product between tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading the training model\")\n",
    "training_model = Model(inputs=, outputs=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading sub-models for retrieving Neural codes\")\n",
    "caption_model = Model(inputs=, outputs=)\n",
    "image_model = Model(inputs=, outputs=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function as a loss for maximizing the margin between a positive and\n",
    "negative example.  If we call $p_i$ the score of the positive pair of the $i$-th example, and $n_i$ the score of the negative pair of that example, the loss is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "loss = \\sum_i{max(0, 1 -p_i + n_i)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def max_margin_loss(y_true, y_pred):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    loss_ =\n",
    "    \n",
    "    return loss_\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metric for max-margin loss\n",
    "How many times did the positive pair effectively get a higher value than the negative pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    accuracy_ =\n",
    "    \n",
    "    return accuracy_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "print (\"compiling the training model\")\n",
    "training_model.compile(optimizer='adam', loss=max_margin_loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation for training the model\n",
    "\n",
    "* adjust the length of captions into fixed maximum length (50 words)\n",
    "* sampling caption for each image, while shuffling the image data\n",
    "* encode captions into integer format based on look-up vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling one caption per image\n",
    "# return image_ids, caption_ids\n",
    "\n",
    "def sampling_img_cap(data):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return image_ids, caption_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform raw text caption into integer sequences of fixed maximum length\n",
    "\n",
    "def prepare_caption(caption_ids, caption_data):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    caption_seqs = \n",
    "    \n",
    "      \n",
    "    return caption_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_caps = []\n",
    "for cap in train_data['caps']:\n",
    "    train_caps.append(cap.decode())\n",
    "\n",
    "val_caps = []\n",
    "for cap in val_data['caps']:\n",
    "    val_caps.append(cap.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_image_ids, train_caption_ids = sampling_img_cap(train_data)\n",
    "val_image_ids, val_caption_ids = sampling_img_cap(val_data)\n",
    "\n",
    "x_caption = prepare_caption(train_caption_ids, train_caps)\n",
    "x_image = train_data['ims'][np.array(train_image_ids)]\n",
    "\n",
    "x_val_caption = prepare_caption(val_caption_ids, val_caps)\n",
    "x_val_image = val_data['ims'][np.array(val_image_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create noise set for negative examples of image-fake caption and dummy output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we do not have real output with labels for training the model. Keras architecture expects labels, so we need to create dummy output -- which is numpy array of zeros. This dummy labels or output is never used since we compute loss function based on margin between positive examples (image-real caption) and negative examples (image-fake caption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "train_noise = \n",
    "val_noise = \n",
    "\n",
    "y_train_labels = \n",
    "y_val_labels = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "X_train = \n",
    "Y_train = \n",
    "X_valid = \n",
    "Y_valid = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# fit the model on training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing models and weight parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "# Save model\n",
    "training_model.save(os.path.join(MODEL_PATH,'image_caption_model.h5'))\n",
    "# Save weight parameters\n",
    "training_model.save_weights(os.path.join(MODEL_PATH, 'weights_image_caption.hdf5'))\n",
    "\n",
    "# Save model for encoding caption and image\n",
    "caption_model.save(os.path.join(MODEL_PATH,'caption_model.h5'))\n",
    "image_model.save(os.path.join(MODEL_PATH,'image_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature extraction (Neural codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Use caption_model and image_model to produce \"Neural codes\" \n",
    "# for both image and caption from validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Caption Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display original image as query and its ground truth caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# choose one image_id from validation set\n",
    "# use this id to get filepath of image\n",
    "img_id = \n",
    "filepath_image = \n",
    "\n",
    "# display original caption\n",
    "original_caption = \n",
    "print(original_caption)\n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "img = image.load_img(os.path.join(IMAGE_DATA,filepath_image), target_size=(224,224))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve caption, given an image query\n",
    "\n",
    "def get_caption(image_filename, n=10):   \n",
    "    \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "get_caption(filepath_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given text query, display retrieved image, similarity score, and its original caption \n",
    "\n",
    "def search_image(text_caption, n=10):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider to use the following settings for image retrieval task.\n",
    "\n",
    "* use real caption that is available in validation set as a query.\n",
    "* use part of caption as query. For instance, instead of use the whole text sentence of the\n",
    "caption, you may consider to use key phrase or combination of words that is included in\n",
    "corresponding caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of text query \n",
    "# text = 'two giraffes standing near trees'\n",
    "\n",
    "# YOUR QUERY-1\n",
    "text1 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR QUERY-2\n",
    "text2 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
