{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5.4 Modeling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter sentiment data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same twitter data set as in Practical-5.3, but we will train the sequence model using word sequences, instead of character sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embedding'\n",
    "MODEL_PATH = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for preprocessing tweets by Romain Paulus\n",
    "# with small modifications by Jeffrey Pennington\n",
    "# from http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\n",
    "def split_hashtag(found):\n",
    "    hashtag_body = found.group(0)[1:]\n",
    "    \n",
    "    return \"<HASHTAG> \" + hashtag_body + \" <ALLCAPS>\"\n",
    "\n",
    "    \n",
    "def preprocess(text):\n",
    "\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "\n",
    "    text = re.sub(r'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*', '<URL>', text)\n",
    "    text = re.sub(r'/', ' / ', text) # Force splitting words appended with slashes (once we tokenized the URLs, of course)\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    text = re.sub(eyes + nose + r'[)dD]+|[(dD]+' + nose + eyes, \"<SMILE>\", text)\n",
    "    text = re.sub(eyes + nose + r'[pP]+', \"<LOLFACE>\", text)\n",
    "    text = re.sub(eyes + nose + r'\\(+|\\)+' + nose + eyes, \"<SADFACE>\", text)\n",
    "    text = re.sub(eyes + nose + r'( \\/|[\\\\|l*])', \"<NEUTRALFACE>\", text)\n",
    "    text = re.sub(r'<3', \"<HEART>\", text)\n",
    "    text = re.sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', \"<NUMBER>\", text)\n",
    "    text = re.sub(r'#\\S+', split_hashtag, text) # Split hashtags on uppercase letters\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1 <REPEAT>', text) # Mark punctuation repetitions (eg. \"!!!\" => \"! <REPEAT>\")\n",
    "    text = re.sub(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2 <ELONG>', text) # Mark elongated words (eg. \"wayyyy\" => \"way <ELONG>\")\n",
    "    #text = re.sub(r'(?<![<A-Z])([^a-z0-9()<>\\'`\\-]){2,}', lambda x: x.group(1).lower() + ' <ALLCAPS>', text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = []\n",
    "raw_labels = []\n",
    "\n",
    "import csv\n",
    "with open(os.path.join(DATA_PATH,'twitter-sentiment.csv'), 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        raw_texts.append(row[3])\n",
    "        raw_labels.append(row[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform labels into categorical form (one hot encoding for multi class output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tita/miniconda3/envs/tfenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2, 'irrelevant': 3}\n",
    "labels = to_categorical(np.asarray([label_mapping[label] for label in raw_labels]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [preprocess(text) for text in raw_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=32)\n",
    "vocab = tokenizer.word_index\n",
    "vocab['<eos>'] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 32) (4000, 4) (1513, 32) (1513, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = data[:4000], labels[:4000]\n",
    "x_val, y_val = data[4000:], labels[4000:]\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained word embedding (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this GloVe file is originally from https://nlp.stanford.edu/projects/glove/ and trained on 2 billion tweets with a vocabulary of 1.2 million word forms. Filter down the dimension of embedding so that it only contains word tokens seen in twitter data we use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load pretrained embedding\n",
    "def load_embedding(vocab, dimension, filename):\n",
    "    print('loading embeddings from \"%s\"' % filename, file=sys.stderr)\n",
    "    embedding = np.zeros((max(vocab.values()) + 1, dimension), dtype=np.float32)\n",
    "    seen = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            tokens = line.strip().split(' ')\n",
    "            if len(tokens) == dimension + 1:\n",
    "                word = tokens[0]\n",
    "                if word in vocab:\n",
    "                    embedding[vocab[word]] = [float(x) for x in tokens[1:]]\n",
    "                    seen.add(word)\n",
    "                    if len(seen) == len(vocab):\n",
    "                        break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading embeddings from \"data/glove.twitter.27B.100d.filtered.txt\"\n"
     ]
    }
   ],
   "source": [
    "weights = load_embedding(vocab, 100, os.path.join(DATA_PATH,'glove.twitter.27B.100d.filtered.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Recurrent Neural Networks (RNN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct LSTM model that uses word sequences as input to learn sentiment polarity of given text. Consider using the following layers:\n",
    "\n",
    "* Input layer\n",
    "* Embedding layer: initialize with pretrained embedding (GloVe)\n",
    "* LSTM layer\n",
    "* Prediction (Dense) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, 32, 100)           1245400   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "prediction_layer (Dense)     (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 1,287,900\n",
      "Trainable params: 42,500\n",
      "Non-trainable params: 1,245,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "sequence_input = Input(shape=(32,), name='input_layer', dtype='int32')\n",
    "embedding_layer = Embedding(len(vocab), 100, weights=[weights], input_length=32, \\\n",
    "                            trainable=False, name='word_embedding')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "lstm_layer = LSTM(64, name='lstm_layer')(embedded_sequences)\n",
    "output_layer = Dense(labels.shape[1], name='prediction_layer', activation='softmax')(lstm_layer)\n",
    "\n",
    "lstm_model = Model(sequence_input, output_layer)\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1513 samples\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.8814 - acc: 0.6685 - val_loss: 0.7644 - val_acc: 0.6880\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.7063 - acc: 0.7295 - val_loss: 0.7099 - val_acc: 0.7125\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.6406 - acc: 0.7463 - val_loss: 0.6734 - val_acc: 0.7409\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.5889 - acc: 0.7717 - val_loss: 0.6664 - val_acc: 0.7422\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.5561 - acc: 0.7853 - val_loss: 0.6630 - val_acc: 0.7442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0407840908>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
