{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5.1 Modeling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data preprocessing for modeling text sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use IMDB review data set to train a Recurrent Neural Networks (RNN) model, by using two (2) type of text sequences as model input: characters and words. Data can be downloaded from https://storage.googleapis.com/trl_data/imdb_dataset.zip. Training set contains 25000 reviews with labels 0 for \"negative\" sentiment and 1 for \"positive\" sentiment. For validation set, the information about binary labels (0 and 1) can be seen in attribute \"id\" of the data set. Number after character '\\_' represents rating score. If rating <5, then the sentiment score is 0 or \"negative\" sentiment. If the rating is greater than 7, then the score is 1 or \"positive\". Otherwise, it is negative (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of (part of) original text in data set:\n",
    "\n",
    "```\n",
    "id\tsentiment\treview\n",
    "\n",
    "\"7759_3\"\t0\t\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger .\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a text (e.g. a movie review), we need to predict whether this review is positive (class label = 1) or negative (class label = 0). We will work with two (2) types of preprocessing to create sequence for our model input: character-level and word-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data preprocessing for text sequence:\n",
    "\n",
    "* Cleaning raw text data\n",
    "    - remove HTML tags\n",
    "    - remove non-informative characters\n",
    "* Tokenizing raw text into array of word tokens (for word-level sequences)\n",
    "* Create vocabulary index: character based and word based look up dictionary index\n",
    "* Transform tokenized text into integer sequences (based on look up vocabulary index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embedding'\n",
    "MODEL_PATH = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create above directories under your current working directory. Download data set provided and locate it in directory 'data' above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean raw text data\n",
    "\n",
    "def striphtml(html):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', html)\n",
    "\n",
    "def clean(s):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATA_PATH,\"trainingData.tsv\"), header=0, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv(os.path.join(DATA_PATH,\"validationData.tsv\"), header=0, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment with MJ i've started listening to his music, watchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this film (\\the greatest filmed opera ever,\\\" didn't I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 80's exploitation, hooray! The pre-credits opening ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment  \\\n",
       "0  5814_8          1   \n",
       "1  2381_9          1   \n",
       "2  7759_3          0   \n",
       "3  3630_4          0   \n",
       "4  9495_8          1   \n",
       "\n",
       "                                                                                                review  \n",
       "0  With all this stuff going down at the moment with MJ i've started listening to his music, watchi...  \n",
       "1  \\The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goe...  \n",
       "2  The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Pr...  \n",
       "3  It must be assumed that those who praised this film (\\the greatest filmed opera ever,\\\" didn't I...  \n",
       "4  Superbly trashy and wondrously unpretentious 80's exploitation, hooray! The pre-credits opening ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of mortality, nostalgia, and loss of innocence it is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster film. It is full of great action scenes, which are on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw it tonight and my child loved it. At one point my k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression that several different screenplays were written, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob life filmed in New Jersey. The story, characters and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0  12311_10   \n",
       "1    8348_2   \n",
       "2    5828_4   \n",
       "3    7186_2   \n",
       "4   12128_7   \n",
       "\n",
       "                                                                                                review  \n",
       "0  Naturally in a film who's main themes are of mortality, nostalgia, and loss of innocence it is p...  \n",
       "1  This movie is a disaster within a disaster film. It is full of great action scenes, which are on...  \n",
       "2  All in all, this is a movie for kids. We saw it tonight and my child loved it. At one point my k...  \n",
       "3  Afraid of the Dark left me with the impression that several different screenplays were written, ...  \n",
       "4  A very accurate depiction of small time mob life filmed in New Jersey. The story, characters and...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this  will create a cleaned version of training set\n",
    "\n",
    "train_docs = []\n",
    "train_labels = []\n",
    "for cont, sentiment in zip(train_data.review, train_data.sentiment):\n",
    "    \n",
    "    doc = clean(striphtml(cont))\n",
    "    doc = doc.lower() \n",
    "    train_docs.append(doc)\n",
    "    train_labels.append(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this  will create a cleaned version of validation set\n",
    "# we also need to extract labels from attribute 'id'\n",
    "\n",
    "valid_docs =[]\n",
    "valid_labels = []\n",
    "i=0\n",
    "for docid,cont in zip(valid_data.id, valid_data.review):\n",
    "    \n",
    "    id_label = docid.split('_')\n",
    "    # if rating >= 7, then assign 1 (positive sentiment) as label\n",
    "    if(int(id_label[1]) >= 7):\n",
    "        valid_labels.append(1)\n",
    "    # else, assign 0 (negative sentiment) as label\n",
    "    else:\n",
    "        valid_labels.append(0)         \n",
    "    doc = clean(striphtml(cont))\n",
    "    doc = doc.lower() \n",
    "    valid_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Build vocabulary index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level vocabulary index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for generating lookup vocabulary index of character-level text sequences, we use characters from both training and validation set -- as compared to preprocessing word sequences (later). This is because the number of unique characters is fewer that unique words in corresponding document corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''\n",
    "for doc in train_docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "for doc in valid_docs:\n",
    "    for s in doc:\n",
    "        txt += s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 71\n"
     ]
    }
   ],
   "source": [
    "chars = set(txt)\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs of character - index of character in look up vocabulary\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# pairs of index of character - character in look up vocabulary\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('0', 1), ('\\x08', 57), ('8', 3), ('c', 70)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(char_indices.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, '0'), (2, '!'), (3, '8'), (4, ')')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(indices_char.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary index\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'char_indices.npy'), char_indices)\n",
    "np.save(os.path.join(DATA_PATH,'indices_char.npy'), indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION to tokenize documents into array list of words\n",
    "# you may also use nltk tokenizer, sklearn tokenizer, or keras tokenizer - \n",
    "# but for the tutorial in text modeling, we will use below function: \n",
    "\n",
    "def tokenizeWords(text):\n",
    "    \n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).split()\n",
    "    return [str(strtokens) for strtokens in tokens]\n",
    "\n",
    "# FUNCTION to create word-level vocabulary index\n",
    "\n",
    "def indexingVocabulary(array_of_words):\n",
    "\n",
    "    wordIndex = list(array_of_words)\n",
    "    \n",
    "    # we will later pad our sequence into fixed length, so\n",
    "    # we will use '0' as the integer index of pad \n",
    "    wordIndex.insert(0,'<pad>')\n",
    "    \n",
    "    # index for word token '<start>' as a starting sign of sequence. We won't use it for this model\n",
    "    # but for the latter model (sequence-to-sequence model)\n",
    "    wordIndex.append('<start>')\n",
    "    \n",
    "    # index for word token '<end>' as an ending sign of sequence. We won't use it for this model\n",
    "    # but for the latter model (sequence-to-sequence model)\n",
    "    wordIndex.append('<end>')\n",
    "    \n",
    "    # index for word token '<unk>' or unknown words (out of vocabulary words) \n",
    "    wordIndex.append('<unk>')\n",
    "    \n",
    "    vocab=dict([(i,wordIndex[i]) for i in range(len(wordIndex))])\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization (for word sequences as model input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create array list of tokenized words and merged array of these word tokens to generate vocabulary index. Notice that we only use 10.000 most frequent words from training set. Out of Vocabulary (OOV) words will be presented as '<unk>' or unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text from training set\n",
    "\n",
    "train_str_tokens = []\n",
    "all_tokens = []\n",
    "for i, text in enumerate(train_docs):\n",
    "    \n",
    "    # this will create our training corpus\n",
    "    train_str_tokens.append(tokenizeWords(text))\n",
    "    \n",
    "    # this will be our merged array to create vocabulary index\n",
    "    all_tokens.extend(tokenizeWords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likewise, tokenize text from validation set\n",
    "\n",
    "valid_str_tokens = []\n",
    "for i, text in enumerate(valid_docs):\n",
    "\n",
    "    valid_str_tokens.append(tokenizeWords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nltk to count word frequency and use 10.000 most frequent words to generate vocabulary index\n",
    "\n",
    "tf = nltk.FreqDist(all_tokens)\n",
    "common_words = tf.most_common(10000)\n",
    "arr_common = np.array(common_words)\n",
    "words = arr_common[:,0]\n",
    "\n",
    "# create vocabulary index\n",
    "\n",
    "# word- index pairs\n",
    "words_indices = indexingVocabulary(words)\n",
    "\n",
    "# index - word pairs\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '<pad>'), (1, 'the'), (2, 'and'), (3, 'a'), (4, 'of')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(words_indices.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order', 652),\n",
       " ('stirring', 8397),\n",
       " ('elementary', 9980),\n",
       " ('duke', 3430),\n",
       " ('unfortunately', 469)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(indices_words.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary index\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'words_indices.npy'), words_indices)\n",
    "np.save(os.path.join(DATA_PATH,'indices_words.npy'), indices_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Preparing model input - output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our maximum length of character sequences as model input equals to 1000 character length. We also need to pad the sequence, in a case when the length of sequence < 1000 characters. Using vocabulary index as our look up dictionary, transform character sequences into integer format of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximum length of input sequence for the model \n",
    "maxlen = 500 # 500 characters length\n",
    "\n",
    "# initialize sequence as numpy array of zeros \n",
    "# will be acted as our padding if text length < 1000 characters\n",
    "X_train = np.zeros((len(train_docs), maxlen), dtype=np.int32)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# transform sequence of characters into their integer format of sequence (based on look up vocabulary index)\n",
    "for i, doc in enumerate(train_docs):\n",
    "    len_doc = len(doc)\n",
    "    if len_doc > maxlen:\n",
    "        txt = doc[:maxlen]\n",
    "    else:\n",
    "        txt = doc\n",
    "    for j, char in enumerate(txt):\n",
    "        X_train[i, j] = char_indices[char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, do similar steps for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.zeros((len(valid_docs), maxlen), dtype=np.int32) \n",
    "y_valid = np.array(valid_labels)\n",
    "\n",
    "for i, doc in enumerate(valid_docs):\n",
    "    len_doc = len(doc)\n",
    "    if len_doc > maxlen:\n",
    "        txt = doc[:maxlen]\n",
    "    else:\n",
    "        txt = doc\n",
    "    for j, char in enumerate(txt):\n",
    "        X_valid[i, j] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'X_train_char.npy'), X_train)\n",
    "np.save(os.path.join(DATA_PATH,'y_train_char.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'X_valid_char.npy'), X_valid)\n",
    "np.save(os.path.join(DATA_PATH,'y_valid_char.npy'), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer format of training input \n",
    "train_int_input = []\n",
    "for i, text in enumerate(train_str_tokens):\n",
    "    int_tokens = [indices_words[w] if w in indices_words.keys() else indices_words['<unk>'] for w in text ]\n",
    "    train_int_input.append(int_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer format of test validation input \n",
    "valid_int_input = []\n",
    "for i, text in enumerate(valid_str_tokens):\n",
    "    int_tokens = [indices_words[w] if w in indices_words.keys() else indices_words['<unk>'] for w in text ]\n",
    "    valid_int_input.append(int_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr = np.array(train_int_input)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "X_valid_arr = np.array(valid_int_input)\n",
    "y_valid = np.array(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding word sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define maximum 500 words as our fixed length of input sequences. Here, we use keras padding, but you may also define your own padding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train_arr, maxlen=max_review_length)\n",
    "X_valid = sequence.pad_sequences(X_valid_arr, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'X_train_word.npy'), X_train)\n",
    "np.save(os.path.join(DATA_PATH,'y_train_word.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'X_valid_word.npy'), X_valid)\n",
    "np.save(os.path.join(DATA_PATH,'y_valid_word.npy'), y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
