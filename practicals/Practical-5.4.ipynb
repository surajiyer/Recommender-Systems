{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5.4 Modeling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter sentiment data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same twitter data set as in Practical-5.3, but we will train the sequence model using word sequences, instead of character sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embedding'\n",
    "MODEL_PATH = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for preprocessing tweets by Romain Paulus\n",
    "# with small modifications by Jeffrey Pennington\n",
    "# from http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\n",
    "def split_hashtag(found):\n",
    "    hashtag_body = found.group(0)[1:]\n",
    "    \n",
    "    return \"<HASHTAG> \" + hashtag_body + \" <ALLCAPS>\"\n",
    "\n",
    "    \n",
    "def preprocess(text):\n",
    "\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "\n",
    "    text = re.sub(r'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*', '<URL>', text)\n",
    "    text = re.sub(r'/', ' / ', text) # Force splitting words appended with slashes (once we tokenized the URLs, of course)\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    text = re.sub(eyes + nose + r'[)dD]+|[(dD]+' + nose + eyes, \"<SMILE>\", text)\n",
    "    text = re.sub(eyes + nose + r'[pP]+', \"<LOLFACE>\", text)\n",
    "    text = re.sub(eyes + nose + r'\\(+|\\)+' + nose + eyes, \"<SADFACE>\", text)\n",
    "    text = re.sub(eyes + nose + r'( \\/|[\\\\|l*])', \"<NEUTRALFACE>\", text)\n",
    "    text = re.sub(r'<3', \"<HEART>\", text)\n",
    "    text = re.sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', \"<NUMBER>\", text)\n",
    "    text = re.sub(r'#\\S+', split_hashtag, text) # Split hashtags on uppercase letters\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1 <REPEAT>', text) # Mark punctuation repetitions (eg. \"!!!\" => \"! <REPEAT>\")\n",
    "    text = re.sub(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2 <ELONG>', text) # Mark elongated words (eg. \"wayyyy\" => \"way <ELONG>\")\n",
    "    #text = re.sub(r'(?<![<A-Z])([^a-z0-9()<>\\'`\\-]){2,}', lambda x: x.group(1).lower() + ' <ALLCAPS>', text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = []\n",
    "raw_labels = []\n",
    "\n",
    "import csv\n",
    "with open(os.path.join(DATA_PATH,'twitter-sentiment.csv'), 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        raw_texts.append(row[3])\n",
    "        raw_labels.append(row[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform labels into categorical form (one hot encoding for multi class output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2, 'irrelevant': 3}\n",
    "labels = to_categorical(np.asarray([label_mapping[label] for label in raw_labels]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [preprocess(text) for text in raw_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=32)\n",
    "vocab = tokenizer.word_index\n",
    "vocab['<eos>'] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data[:4000], labels[:4000]\n",
    "x_val, y_val = data[4000:], labels[4000:]\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained word embedding (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this GloVe file is originally from https://nlp.stanford.edu/projects/glove/ and trained on 2 billion tweets with a vocabulary of 1.2 million word forms. Filter down the dimension of embedding so that it only contains word tokens seen in twitter data we use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load pretrained embedding\n",
    "def load_embedding(vocab, dimension, filename):\n",
    "    print('loading embeddings from \"%s\"' % filename, file=sys.stderr)\n",
    "    embedding = np.zeros((max(vocab.values()) + 1, dimension), dtype=np.float32)\n",
    "    seen = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            tokens = line.strip().split(' ')\n",
    "            if len(tokens) == dimension + 1:\n",
    "                word = tokens[0]\n",
    "                if word in vocab:\n",
    "                    embedding[vocab[word]] = [float(x) for x in tokens[1:]]\n",
    "                    seen.add(word)\n",
    "                    if len(seen) == len(vocab):\n",
    "                        break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = load_embedding(vocab, 100, os.path.join(DATA_PATH,'glove.twitter.27B.100d.filtered.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Recurrent Neural Networks (RNN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct LSTM model that uses word sequences as input to learn sentiment polarity of given text. Consider using the following layers:\n",
    "\n",
    "* Input layer\n",
    "* Embedding layer: initialize with pretrained embedding (GloVe)\n",
    "* LSTM layer\n",
    "* Prediction (Dense) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
