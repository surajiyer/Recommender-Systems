{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5.2 Modeling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Character-level sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB user review data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use character sequences of IMDB text reviews to predict whether the review is positive (class label=1) or negative (class label =0). Download data set from https://storage.googleapis.com/trl_data/imdb_dataset.zip. Run Practical 5.1 to preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embedding'\n",
    "MODEL_PATH = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading stored character-level vocabulary index\n",
    "\n",
    "np_indices_char = np.load(os.path.join(DATA_PATH,'indices_char.npy'))\n",
    "\n",
    "import collections\n",
    "\n",
    "indices_char = collections.OrderedDict()\n",
    "for i in range(len(np_indices_char.item())):\n",
    "    index_val =  np_indices_char.item()[i]\n",
    "    indices_char[i] = index_val\n",
    "    \n",
    "char_indices = dict((c, i) for i, c in (indices_char.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(DATA_PATH,'X_train_char.npy'))\n",
    "y_train = np.load(os.path.join(DATA_PATH,'y_train_char.npy'))\n",
    "\n",
    "X_valid = np.load(os.path.join(DATA_PATH,'X_valid_char.npy'))\n",
    "y_valid = np.load(os.path.join(DATA_PATH,'y_valid_char.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we only use smaller set to train our model \n",
    "# original set consists of 25.000 reviews\n",
    "\n",
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "\n",
    "X_valid = X_valid[5000:6000]\n",
    "y_valid = y_valid[5000:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Character-level Recurrent Neural Networks (RNN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import LSTM, Lambda\n",
    "import tensorflow as tf\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = len(char_indices)\n",
    "max_sequence_length = 500\n",
    "rnn_dim = 100\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(x, sz=num_chars):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], num_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model (Keras sequential model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(binarize, output_shape=binarize_outshape,name='char_embedding', \\\n",
    "                 input_shape=(max_sequence_length,), dtype='int32'))\n",
    "model.add(LSTM(rnn_dim, , name='lstm_layer'))\n",
    "model.add(Dense(1 , name='prediction_layer', activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model (Keras functional API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model architecture, with modularity of Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct architecture\n",
    "input_layer = Input(shape=(max_sequence_length, ), name='input_layer', dtype='int32')\n",
    "char_embedding = Lambda(binarize, output_shape=binarize_outshape,name='char_embedding')(input_layer)\n",
    "lstm_layer = LSTM(rnn_dim, name='lstm_layer')(char_embedding)\n",
    "output_layer = Dense(1, name='prediction_layer', activation='sigmoid')(lstm_layer)\n",
    "\n",
    "# define and load model\n",
    "lstm_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_model.summary()\n",
    "\n",
    "# compile model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Discuss the result of model training. What could be the reason why this model does not converge? \n",
    "Try adding more layers (Dropout, Dense) -- or adding more data, changing hyperparameters, does it help?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
