{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5.3 Modeling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter sentiment data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will use twitter data set, which can be downloaded here: https://storage.googleapis.com/trl_data/twitter_sentiment.zip. Notice that this data set contains shorter text than the text reviews being used in Practical 5.2. Download data and locate in the directory 'data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embedding'\n",
    "MODEL_PATH = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for preprocessing tweets by Romain Paulus\n",
    "# with small modifications by Jeffrey Pennington\n",
    "# from http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\n",
    "def split_hashtag(found):\n",
    "    hashtag_body = found.group(0)[1:]\n",
    "    \n",
    "    return \"<HASHTAG> \" + hashtag_body + \" <ALLCAPS>\"\n",
    "\n",
    "    \n",
    "def preprocess(text):\n",
    "\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "\n",
    "    text = re.sub(r'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*', '<URL>', text)\n",
    "    text = re.sub(r'/', ' / ', text) # Force splitting words appended with slashes (once we tokenized the URLs, of course)\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    text = re.sub(eyes + nose + r'[)dD]+|[(dD]+' + nose + eyes, \"<SMILE>\", text)\n",
    "    text = re.sub(eyes + nose + r'[pP]+', \"<LOLFACE>\", text)\n",
    "    text = re.sub(eyes + nose + r'\\(+|\\)+' + nose + eyes, \"<SADFACE>\", text)\n",
    "    text = re.sub(eyes + nose + r'( \\/|[\\\\|l*])', \"<NEUTRALFACE>\", text)\n",
    "    text = re.sub(r'<3', \"<HEART>\", text)\n",
    "    text = re.sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', \"<NUMBER>\", text)\n",
    "    text = re.sub(r'#\\S+', split_hashtag, text) \n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1 <REPEAT>', text) # Mark punctuation repetitions (eg. \"!!!\" => \"! <REPEAT>\")\n",
    "    text = re.sub(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2 <ELONG>', text) # Mark elongated words (eg. \"wayyyy\" => \"way <ELONG>\")\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = []\n",
    "raw_labels = []\n",
    "\n",
    "import csv\n",
    "with open(os.path.join(DATA_PATH,'twitter-sentiment.csv'), 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        raw_texts.append(row[3])\n",
    "        raw_labels.append(row[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform labels into categorical form (one hot encoding for multi class output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2, 'irrelevant': 3}\n",
    "labels = to_categorical(np.asarray([label_mapping[label] for label in raw_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [preprocess(text) for text in raw_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create character-level vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''\n",
    "for doc in texts:\n",
    "    for s in doc:\n",
    "        txt += s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(txt)\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs of character - index of character in look up vocabulary\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# pairs of index of character - character in look up vocabulary\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only 4000 tweets for training set,\n",
    "# and the rest for validation set\n",
    "\n",
    "train_docs =  texts[:4000]\n",
    "val_docs =  texts[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximum length of input sequence for the model \n",
    "maxlen = 450 # 450 characters length\n",
    "\n",
    "# initialize sequence as numpy array of zeros \n",
    "# will be acted as our padding if text length < 450 characters\n",
    "X_train = np.zeros((len(train_docs), maxlen), dtype=np.int32)\n",
    "y_train = labels[:4000]\n",
    "\n",
    "# transform sequence of characters into their integer format of sequence (based on look up vocabulary index)\n",
    "for i, doc in enumerate(train_docs):\n",
    "    len_doc = len(doc)\n",
    "    if len_doc > maxlen:\n",
    "        txt = doc[:maxlen]\n",
    "    else:\n",
    "        txt = doc\n",
    "    for j, char in enumerate(txt):\n",
    "        X_train[i, j] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.zeros((len(val_docs), maxlen), dtype=np.int32) \n",
    "y_valid = labels[4000:]\n",
    "\n",
    "for i, doc in enumerate(val_docs):\n",
    "    len_doc = len(doc)\n",
    "    if len_doc > maxlen:\n",
    "        txt = doc[:maxlen]\n",
    "    else:\n",
    "        txt = doc\n",
    "    for j, char in enumerate(txt):\n",
    "        X_valid[i, j] = char_indices[char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level Recurrent Neural Networks (RNN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import LSTM, Lambda\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same architecture (Keras functional API) as in Practical-5.2. \n",
    "\n",
    "Note that you train the model to solve multi class classification problem (instead of binary classification in Practical 5.2). So, you need to change slightly the structure of prediction layer (last output layer) and loss function.\n",
    "\n",
    "Does this model suffer a similar problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
