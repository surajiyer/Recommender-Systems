{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5.2 Modeling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Character-level sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB user review data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use character sequences of IMDB text reviews to predict whether the review is positive (class label=1) or negative (class label =0). Download data set from https://storage.googleapis.com/trl_data/imdb_dataset.zip. Run Practical 5.1 to preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embedding'\n",
    "MODEL_PATH = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading stored character-level vocabulary index\n",
    "\n",
    "np_indices_char = np.load(os.path.join(DATA_PATH,'indices_char.npy'))\n",
    "\n",
    "import collections\n",
    "\n",
    "indices_char = collections.OrderedDict()\n",
    "for i in range(len(np_indices_char.item())):\n",
    "    index_val =  np_indices_char.item()[i]\n",
    "    indices_char[i] = index_val\n",
    "    \n",
    "char_indices = dict((c, i) for i, c in (indices_char.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(DATA_PATH,'X_train_char.npy'))\n",
    "y_train = np.load(os.path.join(DATA_PATH,'y_train_char.npy'))\n",
    "\n",
    "X_valid = np.load(os.path.join(DATA_PATH,'X_valid_char.npy'))\n",
    "y_valid = np.load(os.path.join(DATA_PATH,'y_valid_char.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we only use smaller set to train our model \n",
    "# original set consists of 25.000 reviews\n",
    "\n",
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "\n",
    "X_valid = X_valid[5000:6000]\n",
    "y_valid = y_valid[5000:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Character-level Recurrent Neural Networks (RNN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tita/miniconda3/envs/tfenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import LSTM, Lambda\n",
    "import tensorflow as tf\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = len(char_indices)\n",
    "max_sequence_length = 500\n",
    "rnn_dim = 32\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(x, sz=num_chars):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], num_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model (Keras sequential model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_embedding (Lambda)      (None, 500, 71)           0         \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 32)                13312     \n",
      "_________________________________________________________________\n",
      "prediction_layer (Dense)     (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,345\n",
      "Trainable params: 13,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(binarize, output_shape=binarize_outshape,name='char_embedding', \\\n",
    "                 input_shape=(max_sequence_length,), dtype='int32'))\n",
    "model.add(LSTM(rnn_dim, name='lstm_layer'))\n",
    "model.add(Dense(1 , name='prediction_layer', activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 58s 12ms/step - loss: 0.6930 - acc: 0.5032 - val_loss: 0.6894 - val_acc: 0.5350\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 60s 12ms/step - loss: 0.6921 - acc: 0.5192 - val_loss: 0.6901 - val_acc: 0.5180\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 59s 12ms/step - loss: 0.6918 - acc: 0.5162 - val_loss: 0.6899 - val_acc: 0.5180\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 59s 12ms/step - loss: 0.6912 - acc: 0.5266 - val_loss: 0.6899 - val_acc: 0.5230\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 59s 12ms/step - loss: 0.6909 - acc: 0.5264 - val_loss: 0.6897 - val_acc: 0.5190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2418f370f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model (Keras functional API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model architecture, with modularity of Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "char_embedding (Lambda)      (None, 500, 71)           0         \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 32)                13312     \n",
      "_________________________________________________________________\n",
      "prediction_layer (Dense)     (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,345\n",
      "Trainable params: 13,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# construct architecture\n",
    "input_layer = Input(shape=(max_sequence_length, ), name='input_layer', dtype='int32')\n",
    "char_embedding = Lambda(binarize, output_shape=binarize_outshape,name='char_embedding')(input_layer)\n",
    "lstm_layer = LSTM(rnn_dim, name='lstm_layer')(char_embedding)\n",
    "output_layer = Dense(1, name='prediction_layer', activation='sigmoid')(lstm_layer)\n",
    "\n",
    "# define and load model\n",
    "lstm_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_model.summary()\n",
    "\n",
    "# compile model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 63s 13ms/step - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6910 - val_acc: 0.5130\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 57s 11ms/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6910 - val_acc: 0.5080\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 58s 12ms/step - loss: 0.6915 - acc: 0.5240 - val_loss: 0.6908 - val_acc: 0.5260\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 61s 12ms/step - loss: 0.6910 - acc: 0.5324 - val_loss: 0.6906 - val_acc: 0.5220\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 62s 12ms/step - loss: 0.6907 - acc: 0.5290 - val_loss: 0.6908 - val_acc: 0.5290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24151a0048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Discuss the result of model training. What could be the reason why this model does not converge? \n",
    "Try adding more layers (Dropout, Dense) -- or adding more data, changing hyperparameters, does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "char_embedding (Lambda)      (None, 500, 71)           0         \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 32)                13312     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "prediction_layer (Dense)     (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# construct architecture\n",
    "input_layer = Input(shape=(max_sequence_length, ), name='input_layer', dtype='int32')\n",
    "char_embedding = Lambda(binarize, output_shape=binarize_outshape,name='char_embedding')(input_layer)\n",
    "lstm_layer = LSTM(rnn_dim, name='lstm_layer')(char_embedding)\n",
    "output = Dropout(0.5)(lstm_layer)\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output_layer = Dense(1, name='prediction_layer', activation='sigmoid')(output)\n",
    "\n",
    "# define and load model\n",
    "lstm_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_model.summary()\n",
    "\n",
    "# compile model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 64s 13ms/step - loss: 0.6938 - acc: 0.5012 - val_loss: 0.6921 - val_acc: 0.5090\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 62s 12ms/step - loss: 0.6934 - acc: 0.5004 - val_loss: 0.6915 - val_acc: 0.5260\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 84s 17ms/step - loss: 0.6930 - acc: 0.5164 - val_loss: 0.6916 - val_acc: 0.5170\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 59s 12ms/step - loss: 0.6923 - acc: 0.5176 - val_loss: 0.6913 - val_acc: 0.5020\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 64s 13ms/step - loss: 0.6926 - acc: 0.5126 - val_loss: 0.6913 - val_acc: 0.5190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2405d7d550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model simply cannot capture high level abstraction (sentiment polarity) from character sequences.\n",
    "\n",
    "Think how sentiment polarity is conveyed in this type of text reviews. \n",
    "What factors could play important role in capturing sentiment of corresponding text?\n",
    "\n",
    "Can we do better with shorter text?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
